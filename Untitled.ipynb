{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7d7e2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ae57e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise repositories\n",
    "_projroot = os.path.abspath('.')\n",
    "_datadir = os.path.join(_projroot, 'data')\n",
    "_preprocesseddir = os.path.join(_datadir, 'preprocesseddata')\n",
    "_experimentdir = os.path.join(_datadir, 'experiments')\n",
    "_rawdir = os.path.join(_datadir, 'rawdata')\n",
    "_src = os.path.join(_projroot,'src')\n",
    "_sampling = os.path.join(_experimentdir,'sampling')\n",
    "splitsampling = os.path.join(_sampling,'splitsampling')\n",
    "modelsdir = os.path.join(splitsampling,'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "965a9742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'split_data_4_2_model': LogisticRegression(C=1, max_iter=1000), 'split_data_4_4_model': LogisticRegression(C=1, max_iter=1000), 'split_data_4_3_model': LogisticRegression(C=0.1, max_iter=1000), 'split_data_4_1_model': LogisticRegression(C=1, max_iter=1000)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Import pickle files\n",
    "models = {}\n",
    "for filename in os.listdir(modelsdir):\n",
    "    if filename.startswith('split_data_4'):\n",
    "        models[os.path.splitext(filename)[0]] = joblib.load(os.path.join(modelsdir,filename))\n",
    "\n",
    "#k max is max number of classifiers added to adaboost\n",
    "#W_1(i) is the error associated with the ith sample in classifier 1\n",
    "#Pick weak classifier with smallest error\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d07819b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training files\n",
    "features = pd.read_excel(os.path.join(splitsampling, 'splitsampling_features.xlsx'), sheet_name = 'train_test_split')\n",
    "target = pd.read_excel(os.path.join(splitsampling, 'splitsampling_target.xlsx'), sheet_name = 'train_test_split')\n",
    "features_test = pd.read_excel(os.path.join(splitsampling, 'splitsampling_features.xlsx'), sheet_name = 'test_data')\n",
    "target_test = pd.read_excel(os.path.join(splitsampling, 'splitsampling_target.xlsx'), sheet_name = 'test_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0bfe7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop(\"Unnamed: 0\", axis = 1)\n",
    "target = target.values[:,1]\n",
    "total_positives = np.sum(target)\n",
    "total_negatives = len(target) - np.sum(target)\n",
    "target = np.where(target == 0, -1, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b75914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "824f1863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'split_data_4_2_model': 0.07999999999999882, 'split_data_4_4_model': 0.09655172413792967, 'split_data_4_3_model': 0.06758620689655068, 'split_data_4_1_model': 0.07999999999999882}\n",
      "k = 7.869428826768949e-06\n",
      "alpha 2.683957074893497\n",
      "{'split_data_4_2_model': LogisticRegression(C=1, max_iter=1000), 'split_data_4_4_model': LogisticRegression(C=1, max_iter=1000), 'split_data_4_1_model': LogisticRegression(C=1, max_iter=1000)}\n",
      "{'split_data_4_2_model': 0.7862218847286935, 'split_data_4_4_model': 0.7871376999796423, 'split_data_4_1_model': 0.9490959356347581}\n",
      "k = 7.869428826768949e-06\n",
      "alpha 0.7206204778855557\n",
      "{'split_data_4_4_model': LogisticRegression(C=1, max_iter=1000), 'split_data_4_1_model': LogisticRegression(C=1, max_iter=1000)}\n",
      "{'split_data_4_4_model': 0.8176277478353855, 'split_data_4_1_model': 0.9874673732192502}\n",
      "k = 7.869428826768949e-06\n",
      "alpha 0.6215920773018113\n",
      "{'split_data_4_1_model': LogisticRegression(C=1, max_iter=1000)}\n",
      "{'split_data_4_1_model': 0.9963522569208207}\n",
      "k = 7.869428826768949e-06\n",
      "alpha -1.4332254264673925\n",
      "{}\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.]\n",
      "{'split_data_4_1_model': 0.9963522569208207}\n"
     ]
    }
   ],
   "source": [
    "errors = {}\n",
    "k_max = 4\n",
    "W = np.ones(len(target)) * (1/len(target))\n",
    "classifier = 0\n",
    "b = total_negatives/total_positives\n",
    "beta = 0.5\n",
    "for p in range(0,4):\n",
    "    errors = {}\n",
    "    Ws_new = {}\n",
    "    for key in models:\n",
    "        y_pred = models[key].predict(features)\n",
    "        y_pred = np.where(y_pred == 0, -1, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(target, y_pred, labels = [1., -1.]).ravel()\n",
    "        gamma = tp/ total_positives\n",
    "        error = 0\n",
    "        W_tot = 0\n",
    "        for i in range(0, len(target)):\n",
    "            W_tot = W_tot + W[i]\n",
    "            if (target[i]+y_pred[i]) == 0:\n",
    "                error = error + W[i]\n",
    "        errors[key] = error/W_tot\n",
    "        \n",
    "        \n",
    "        #print(W_new)\n",
    "        \n",
    "    #if gamma > 0.5 or errors[key] < 0.5*((1-(2*gamma-1))/b+1):\n",
    "    print(errors)\n",
    "    min_val = min(errors.values())\n",
    "    min_key = [k for k, v in errors.items() if v == min_val]\n",
    "    min_model = models[min_key[0]]\n",
    "        \n",
    "    logplus = 1 + (2*gamma - 1)/(b+1)\n",
    "    logminus = 1 - (2*gamma -1)/b+1\n",
    "    logtot = logplus/logminus\n",
    "    k = 0.5*(np.log(logtot))/(np.exp(beta*(2*gamma-1)))\n",
    "    print('k =', k)                                                             \n",
    "             \n",
    "    alpha = 0.5*np.log((1-errors[min_key[0]])/(errors[min_key[0]])) + k*np.exp(beta*(2*gamma-1))\n",
    "    print('alpha',alpha)\n",
    "    y_pred_min = min_model.predict(features)\n",
    "    y_pred_min = np.where(y_pred_min == 0, -1, y_pred_min)\n",
    "    \n",
    "    W_new = np.array([])\n",
    "    for i in range(0, len(target)):\n",
    "        W_new = np.append(W_new,(W[i]*np.exp(-(alpha*target[i]*y_pred[i]))))\n",
    "    W = W_new\n",
    "    classifier = classifier + alpha*y_pred_min\n",
    "    del models[min_key[0]]\n",
    "    print(models)\n",
    "print(np.sign(classifier))\n",
    "    \n",
    "    \n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "97d82530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sign(classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f6a10948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "classifier = np.where(np.sign(classifier) == -1, 0, np.sign(classifier))\n",
    "target = np.where(target == -1, 0, target)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4f5881d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5702711351602956\n",
      "0.22857142857142856\n",
      "0.1568627450980392\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, roc_auc_score\n",
    "print(roc_auc_score(target, classifier))\n",
    "print(f1_score(target, classifier))\n",
    "print(recall_score(target, classifier))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c296f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
